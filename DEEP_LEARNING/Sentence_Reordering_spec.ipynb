{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ElNaMbLnRdHR"
      },
      "source": [
        "# Sentence Reconstruction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oXr4iGUGRms8"
      },
      "source": [
        "The purpose of this project is to take in input a sequence of words corresponding to a random permutation of a given english sentence, and reconstruct the original sentence. \n",
        "\n",
        "The otuput can be either produced in a single shot, or through an iterative (autoregressive) loop generating a single token at a time.\n",
        "\n",
        "CONSTRAINTS:\n",
        "* No pretrained model can be used.\n",
        "* The neural network models should have less the 20M parameters.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ8k-L-WUK7l"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "The dataset is composed by a snapshot of wikipedia. We restricted the vocabolary to the 10K most frequent words, and only took sentences making use of this vocabulary. In addition, we restricted to sequences with a length between 3 and 30 words.\n",
        "\n",
        "(Ignore the error, if any) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0xmXLLfaUKA6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (11.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (0.3.1.1)\n",
            "Requirement already satisfied: pandas in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (1.5.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (2023.5.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (0.15.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: responses<0.19 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "\n",
            "The conflict is caused by:\n",
            "    datasets 2.12.0 depends on dill<0.3.7 and >=0.3.0\n",
            "    multiprocess 0.70.14 depends on dill>=0.3.6\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000024EFC34F370>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/dill/\n",
            "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000024EFC34F6A0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/dill/\n",
            "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000024EFC34F850>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/dill/\n",
            "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000024EFC34FA00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/dill/\n",
            "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000024EFC34FBB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/dill/\n",
            "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000024EFC3691F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/multiprocess/\n",
            "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000024EFC369580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/multiprocess/\n",
            "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000024EFC369730>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/multiprocess/\n",
            "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000024EFC3698E0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/multiprocess/\n",
            "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000024EFC369A90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/multiprocess/\n",
            "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000024EFC2D0250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/datasets/\n",
            "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000024EFC2F0070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/datasets/\n",
            "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000024EFC2DD4F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/datasets/\n",
            "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000024EFC2DDE20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/datasets/\n",
            "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000024EFC2DD7C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/datasets/\n",
            "ERROR: Cannot install datasets==2.12.0 and multiprocess==0.70.14 because these package versions have conflicting dependencies.\n",
            "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: apache-beam in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (2.48.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (1.7)\n",
            "Requirement already satisfied: orjson<4.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (3.9.0)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (0.3.1.1)\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (1.7.4)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (0.18)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (1.51.1)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (2.7.0)\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (0.22.0)\n",
            "Requirement already satisfied: numpy<1.25.0,>=1.14.3 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (1.22.4)\n",
            "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (0.6.1)\n",
            "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (4.3.3)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (1.22.2)\n",
            "Requirement already satisfied: protobuf<4.24.0,>=3.20.3 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (4.23.1)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (2022.7)\n",
            "Requirement already satisfied: regex>=2020.6.8 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (2023.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (2.28.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (4.4.0)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (0.21.0)\n",
            "Requirement already satisfied: pyarrow<12.0.0,>=3.0.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from apache-beam) (11.0.0)\n",
            "Requirement already satisfied: docopt in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam) (0.6.2)\n",
            "Requirement already satisfied: six>=1.9.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from httplib2<0.23.0,>=0.8->apache-beam) (3.0.9)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pymongo<5.0.0,>=3.8.0->apache-beam) (2.3.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3.0.0,>=2.24.0->apache-beam) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3.0.0,>=2.24.0->apache-beam) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3.0.0,>=2.24.0->apache-beam) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danis\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3.0.0,>=2.24.0->apache-beam) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip3 install apache-beam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "INZIMG8itLHh"
      },
      "outputs": [],
      "source": [
        "from random import Random\n",
        "\n",
        "# Instantiate the Random instance with random seed = 42 to ensure reproducibility\n",
        "randomizer = Random(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jRVmQCKdRb54"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical, pad_sequences\n",
        "import pickle\n",
        "import random\n",
        "import logging\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AoeyVDv9uDwx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\danis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Using the latest cached version of the module from C:\\Users\\danis\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\wikipedia\\aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559 (last modified on Thu Jun  8 18:07:07 2023) since it couldn't be found locally at wikipedia., or remotely on the Hugging Face Hub.\n",
            "Found cached dataset wikipedia (C:/Users/danis/.cache/huggingface/datasets/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n",
            "100%|██████████| 1/1 [00:00<00:00, 37.96it/s]\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
        "\n",
        "data = dataset['train'][:20000]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzcYlWm8trh9",
        "outputId": "9e3f30fb-8464-43df-b066-0cd112c74972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "corpus dim:  510023\n",
            "filtered sentences:  137301\n"
          ]
        }
      ],
      "source": [
        "#run this cell only the first time to create and save the tokenizer and the date\n",
        "dump = True\n",
        "\n",
        "tokenizer = Tokenizer(split=' ', filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', num_words=10000, oov_token='<unk>')\n",
        "\n",
        "corpus = []\n",
        "\n",
        "# Split of each piece of text into sentences\n",
        "for elem in data:\n",
        "  corpus += elem.lower().replace(\"\\n\", \"\").split(\".\")[:]\n",
        "\n",
        "print(\"corpus dim: \",len(corpus))\n",
        "\n",
        "#add a start and an end token\n",
        "corpus = ['<start> '+s+' <end>' for s in corpus]\n",
        "\n",
        "# Tokenization\t\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "if dump:\n",
        "    with open('tokenizer.pickle', 'wb') as handle:\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "original_data = [sen for sen in tokenizer.texts_to_sequences(corpus) if (len(sen) <= 32 and len(sen)>4 and not(1 in sen))]\n",
        "\n",
        "if dump:\n",
        "    with open('original.pickle', 'wb') as handle:\n",
        "        pickle.dump(original_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print (\"filtered sentences: \",len(original_data))\n",
        "\n",
        "sos = tokenizer.word_index['<start>']\n",
        "eos = tokenizer.word_index['<end>']\n",
        "\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K1woGS7a4Ez4"
      },
      "source": [
        "We now create two additional datasets. \n",
        "* shuffled_data contains scrumbled sequences, and will be the input to the model. \n",
        "* target_data is the same as original data but offset by one timestep.\n",
        "It is only useful if you plan to do some language modeling with a teacher forcing technique. You might decide to ignore it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rs4cerfa4D15"
      },
      "outputs": [],
      "source": [
        "shuffled_data = [random.sample(s[1:-1],len(s)-2) for s in original_data]\n",
        "shuffled_data = [[sos]+s+[eos] for s in shuffled_data]\n",
        "target_data = [s[1:] for s in original_data]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mGNwATns6hQ0"
      },
      "source": [
        "Let us look at some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ChbvR6ue6lpj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original sentence:  [2, 58, 16, 193, 4581, 166, 38, 85, 614, 9, 830, 13, 20, 3798, 99, 1798, 3]\n",
            "shuffled sentecen:  [2, 99, 830, 1798, 9, 13, 614, 85, 193, 166, 16, 58, 3798, 38, 4581, 20, 3]\n"
          ]
        }
      ],
      "source": [
        "i = np.random.randint(len(original_data))\n",
        "print(\"original sentence: \",original_data[i])\n",
        "print(\"shuffled sentecen: \",shuffled_data[i])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "er0FoTdc8sLq"
      },
      "source": [
        "Let us look at detokenized data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OMKM9B1w8yWX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original sentence:  <start> 8 protestant 35 <end>\n",
            "shuffled sentence:  <start> protestant 35 8 <end>\n"
          ]
        }
      ],
      "source": [
        "i = np.random.randint(len(original_data))\n",
        "print(\"original sentence: \",tokenizer.sequences_to_texts([original_data[i]])[0])\n",
        "print(\"shuffled sentence: \",tokenizer.sequences_to_texts([shuffled_data[i]])[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Kja87gEg9Rje"
      },
      "source": [
        "You goal is to reconstruct the original sentence out of the shuffled one."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s6pe2f8h9gmG"
      },
      "source": [
        "# Additional material"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EA6su74d9o7v"
      },
      "source": [
        "Here we provide a few additional functions that could be useful to you."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MhD75oyt-AO8"
      },
      "source": [
        "As usual, you are supposed to divide your data in training and test set. Reserve at least 30% of data for testing.\n",
        "\n",
        "You are likely to need a validation set too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dIDuV_Sj9oZo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, c_train, c_test, y_train, y_test = train_test_split(shuffled_data, original_data, target_data, test_size = 0.3, random_state = 42)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eOjaBx8d-lEw"
      },
      "source": [
        "Depending from the model you plan to build, you might require padding the input sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cbZ1tSFN-kWj"
      },
      "outputs": [],
      "source": [
        "max_sequence_len = max([len(x) for x in original_data])\n",
        "\n",
        "x_train = pad_sequences(x_train, maxlen=max_sequence_len, padding='post')\n",
        "x_test = pad_sequences(x_test, maxlen=max_sequence_len, padding='post')\n",
        "c_train = pad_sequences(c_train, maxlen=max_sequence_len, padding='post')\n",
        "c_test = pad_sequences(c_test, maxlen=max_sequence_len, padding='post')\n",
        "y_train = pad_sequences(y_train, maxlen=max_sequence_len, padding='post')\n",
        "y_test = pad_sequences(y_test, maxlen=max_sequence_len, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2PVzEwm8-8Yj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train size: 96110\n"
          ]
        }
      ],
      "source": [
        "print(\"x_train size:\", len(x_train))\n",
        "assert(len(x_train)==len(c_train)==len(y_train))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4jrATEiF_mMo"
      },
      "source": [
        "Let us finally have a look at the distribution of data w.r.t. their lenght."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KmzOMET9_jxp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([ 3897.,  5516.,  6180.,  7633., 10474., 11260., 11167., 10501.,\n",
              "         9768.,  8942.,  7828.,  7010.,  6126.,  5236.,  4551.,  3922.,\n",
              "         3260.,  2695.,  2306.,  1922.,  1611.,  1299.,  1126.,   827.,\n",
              "          773.,   586.,   885.]),\n",
              " array([ 3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15.,\n",
              "        16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28.,\n",
              "        29., 30.]),\n",
              " <BarContainer object of 27 artists>)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist([len(x)-2 for x in original_data],27)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Metrics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let s be the source string and p your prediction. The quality of the results will be measured according to the following metric:\n",
        "\n",
        "1.  look for the longest substring w between s and p\n",
        "2.  compute |w|/|s|\n",
        "\n",
        "If the match is exact, the score is 1. \n",
        "\n",
        "When computing the score, you should NON consider the start and end tokens.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The longest common substring can be computed with the SequenceMatcher function of difflib, that allows a simple definition of our metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def score_fun(s,p):\n",
        "  match = SequenceMatcher(None, s, p).find_longest_match()\n",
        "  #print(match.size)\n",
        "  return (match.size/max(len(p),len(s)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's do an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original = \"at first henry wanted to be friends with the king of france\"\n",
        "generated = \"henry wanted to be friends with king of france at the first\"\n",
        "\n",
        "print(\"your score is \",score_fun(original,generated))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The score must be computed as an average of at least 3K random examples taken form the test set."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Daniele Simonazzi - 0001058929"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer vs Seq2Seq"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To solve this problem, I had to choose between a Seq2Seq model and a transformer model. The decision was actually pretty easy since I considered the limited amount of time available and the several following benefits of the transformer:\n",
        "- Self-attention mechanism that allows it to focus on different parts of the input sequence during encoding and decoding phases\n",
        "- Parallel processing that enables faster training and inference times\n",
        "- Capability to capture long-range dependencies (due to attention and positional encoding), which eliminates the struggle of a Seq2Seq model when the distance between relevant words is significant"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see the structure of the implemented transformer:\n",
        "\n",
        "![Transformer structure](https://www.tensorflow.org/images/tutorials/transformer/Transformer-4layer-compact.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_val = x_train[-1500:]\n",
        "c_val = c_train[-1500:]\n",
        "y_val = y_train[-1500:]\n",
        "\n",
        "x_train = x_train[:-1500]\n",
        "c_train = c_train[:-1500]\n",
        "y_train = y_train[:-1500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "BATCHES_PER_EPOCH = int(np.floor(x_train.shape[0]/BATCH_SIZE))\n",
        "BATCHES_PER_EPOCH_VAL = int(np.floor(x_val.shape[0]/BATCH_SIZE))\n",
        "\n",
        "class BatchIterator:\n",
        "    def __init__(self, x_shuffled, x_original, y_shifted, batch_size, batches_per_epoch):\n",
        "        self.x_shuffled = x_shuffled\n",
        "        self.x_original = x_original\n",
        "        self.y_shifted = y_shifted\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.batches_per_epoch = batches_per_epoch\n",
        "        self.list_batches = []\n",
        "         \n",
        "        self._initialize_batches()\n",
        "        \n",
        "    def _initialize_batches(self):\n",
        "        indices = [ii for ii in range(self.x_shuffled.shape[0])]\n",
        "        random.shuffle(indices)\n",
        "        for ii in range(self.batches_per_epoch):\n",
        "            self.list_batches.append(indices[self.batch_size*ii : self.batch_size*(ii+1)])\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if len(self.list_batches) == 0:\n",
        "            self._initialize_batches()\n",
        "\n",
        "        current_batch_indeces = self.list_batches.pop()\n",
        "\n",
        "        batch_shuffled = self.x_shuffled[current_batch_indeces]\n",
        "        batch_original = self.x_original[current_batch_indeces]\n",
        "        batch_shifted = self.y_shifted[current_batch_indeces]\n",
        "\n",
        "        return ([batch_shuffled,batch_original], batch_shifted)\n",
        "    \n",
        "MyBatchIterator_train = BatchIterator(x_train, c_train, y_train, BATCH_SIZE, BATCHES_PER_EPOCH)\n",
        "MyBatchIterator_val = BatchIterator(x_val, c_val, y_val, BATCH_SIZE, BATCHES_PER_EPOCH_VAL)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positional encoding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Both the encoder and decoder use the same logic for embedding and positional encoding of inputs.   \n",
        "To convert a sequence of tokens into vectors, both input and target tokens must pass through a tf.keras.layers.Embedding layer.\n",
        "Since Transformers make no assumptions about the temporal/spatial relationships across the data we need to convey this information through \"Positional Encoding\".  \n",
        "Hence, a word is represented by a vector which is the sum of a traditional encoding and a positional one. \n",
        "Throughout the model this mechanism allow attention layers to receive a set of vectors as input, with no specific order.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1) \n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s use this to create a PositionalEmbedding layer that looks-up a token's embedding vector and adds the position vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add and Normalize"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"Add & Norm\" blocks are scattered throughout the model. Each one joins a residual connection and runs the result through a LayerNormalization layer. The residual \"Add & Norm\" blocks are included so that training is efficient"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base attention layer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Attention layers are used throughout the model. These are all identical except for how the attention is configured.  \n",
        "Each one contains a layers.MultiHeadAttention, a layers.LayerNormalization and a layers.Add.   \n",
        "To implement these attention layers, let's start with a simple base class that just contains the component layers. Each use-case will be implemented as a subclass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The cross attention layer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At the literal center of the Transformer is the cross-attention layer. This layer connects the encoder and decoder.  \n",
        "To implement this you pass the target sequence x (from the decoder) as the query and the context sequence (from the encoder) as the key/value when calling the multi-head-attention layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Global self attention layer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This layer is responsible for processing the context sequence, and propagating information along its length.  \n",
        "To implement this layer you just need to pass the target sequence, x, as both the query, and value arguments to the multi-head-attention layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Causal self attention layer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This layer does a similar job as the global self attention layer, for the output sequence.  \n",
        "Let's recall that Transformers are an \"autoregressive\" model: meaning that they generate the text one token at a time and feed that output back to the input.  \n",
        "To make this efficient, these models ensure that the output for each sequence element only depends on the previous sequence elements; the models are \"causal\".  \n",
        "To build a causal self attention layer, you need to use an appropriate mask when computing the attention scores and summing the attention values.\n",
        "This is taken care of automatically if you pass use_causal_mask = True to the MultiHeadAttention layer when you call it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feed forward network"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The transformer also includes a point-wise feed-forward network in both the encoder and decoder.  \n",
        "The network consists of two linear layers tf.keras.layers.Dense with a ReLU activation in-between, and a dropout layer. As with the attention layers the code here also includes the residual connection and normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x) \n",
        "    return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The encoder layer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The encoder contains a stack of N encoder layers. Where each EncoderLayer contains a GlobalSelfAttention and FeedForward layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoder"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s now build the encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The decoder layer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The decoder's stack is slightly more complex, with each DecoderLayer containing a CausalSelfAttention, a CrossAttention, and a FeedForward layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decoder"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s now build the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the end, in order to complete the Transformer model, the Encoder and the Decoder have to be put together and a final linear (Dense) layer which converts the resulting vector at each location into output token probabilities has to be added.  \n",
        "The output of the decoder is the input to this final linear layer.  \n",
        "Let's notice that actually the Transformer model output the logits and not the probabilities, however this will not create a problem since SparseCategoricalEntropyLoss provided by Keras can smoothly handle this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    context, x  = inputs\n",
        "\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "      # b/250038731\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_layers = 4 # the number of layers\n",
        "d_model = 128 # the dimensionality of the embeddings\n",
        "dff = 512 # internal dimensionality of the FeedForward layer\n",
        "num_heads = 8 # self-attention heads\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instantiate the Transformer model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=10000,\n",
        "    target_vocab_size=10000,\n",
        "    dropout_rate=dropout_rate)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set-up the optimizer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It s used an Adam optimizer where the learning rate varies over the course of training.  \n",
        "It increases the learning rate linearly for the first warmup_steps training steps,\n",
        "and decreases it thereafter proportionally to the inverse square root of the step number. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "  \n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set up the loss and metrics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss.  \n",
        "The cross-entropy loss function tf.keras.losses.SparseCategoricalCrossentropy is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define a callback such that best weights and biases are dinamically stored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    os.path.join(\"transformer_weights_and_biases.h5\"),    # Path to save the best weights\n",
        "    monitor='val_loss',   # Metric to monitor for saving the best weights\n",
        "    save_best_only=True,  # Save only the best weights\n",
        "    save_weights_only=True  # Save only the weights, not the entire model\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And let's plot the comparisons b/w training and validation loss and b/w  training and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_history(history):\n",
        "    loss_training = history.history['loss']\n",
        "    acc_training = history.history['masked_accuracy']\n",
        "\n",
        "    loss_val = history.history['val_loss']\n",
        "    acc_val = history.history['val_masked_accuracy']\n",
        "\n",
        "    # Visualize the behavior of the loss\n",
        "    plt.plot(loss_training)\n",
        "    plt.plot(loss_val)\n",
        "    plt.grid()\n",
        "    plt.title('Loss during training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.show()\n",
        "\n",
        "    # and of the accuracy\n",
        "    plt.plot(acc_training)\n",
        "    plt.plot(acc_val)\n",
        "    plt.grid()\n",
        "    plt.title('Accuracy during training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s first compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then let's train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1478/1478 [==============================] - 1745s 1s/step - loss: 6.1835 - masked_accuracy: 0.2306 - val_loss: 4.3071 - val_masked_accuracy: 0.3919\n",
            "Epoch 2/20\n",
            "1478/1478 [==============================] - 1723s 1s/step - loss: 3.5831 - masked_accuracy: 0.4649 - val_loss: 2.7570 - val_masked_accuracy: 0.5612\n",
            "Epoch 3/20\n",
            "1478/1478 [==============================] - 1718s 1s/step - loss: 2.5303 - masked_accuracy: 0.5623 - val_loss: 2.1111 - val_masked_accuracy: 0.6154\n",
            "Epoch 4/20\n",
            "1478/1478 [==============================] - 1718s 1s/step - loss: 1.9184 - masked_accuracy: 0.6222 - val_loss: 1.6948 - val_masked_accuracy: 0.6608\n",
            "Epoch 5/20\n",
            "1478/1478 [==============================] - 1724s 1s/step - loss: 1.5524 - masked_accuracy: 0.6661 - val_loss: 1.5195 - val_masked_accuracy: 0.6858\n",
            "Epoch 6/20\n",
            "1478/1478 [==============================] - 1720s 1s/step - loss: 1.3368 - masked_accuracy: 0.6957 - val_loss: 1.4187 - val_masked_accuracy: 0.6998\n",
            "Epoch 7/20\n",
            "1478/1478 [==============================] - 1720s 1s/step - loss: 1.1921 - masked_accuracy: 0.7177 - val_loss: 1.3166 - val_masked_accuracy: 0.7161\n",
            "Epoch 8/20\n",
            "1478/1478 [==============================] - 1723s 1s/step - loss: 1.0798 - masked_accuracy: 0.7362 - val_loss: 1.2979 - val_masked_accuracy: 0.7184\n",
            "Epoch 9/20\n",
            "1478/1478 [==============================] - 1718s 1s/step - loss: 0.9922 - masked_accuracy: 0.7513 - val_loss: 1.2619 - val_masked_accuracy: 0.7195\n",
            "Epoch 10/20\n",
            "1478/1478 [==============================] - 1720s 1s/step - loss: 0.9214 - masked_accuracy: 0.7640 - val_loss: 1.2332 - val_masked_accuracy: 0.7328\n",
            "Epoch 11/20\n",
            "1478/1478 [==============================] - 1719s 1s/step - loss: 0.8610 - masked_accuracy: 0.7765 - val_loss: 1.2485 - val_masked_accuracy: 0.7305\n",
            "Epoch 12/20\n",
            "1478/1478 [==============================] - 1720s 1s/step - loss: 0.8095 - masked_accuracy: 0.7862 - val_loss: 1.2310 - val_masked_accuracy: 0.7402\n",
            "Epoch 13/20\n",
            "1478/1478 [==============================] - 1719s 1s/step - loss: 0.7696 - masked_accuracy: 0.7943 - val_loss: 1.2274 - val_masked_accuracy: 0.7413\n",
            "Epoch 14/20\n",
            "1478/1478 [==============================] - 1719s 1s/step - loss: 0.7270 - masked_accuracy: 0.8029 - val_loss: 1.2471 - val_masked_accuracy: 0.7400\n",
            "Epoch 15/20\n",
            "1478/1478 [==============================] - 1724s 1s/step - loss: 0.6902 - masked_accuracy: 0.8112 - val_loss: 1.2118 - val_masked_accuracy: 0.7481\n",
            "Epoch 16/20\n",
            "1478/1478 [==============================] - 1719s 1s/step - loss: 0.6593 - masked_accuracy: 0.8181 - val_loss: 1.2704 - val_masked_accuracy: 0.7413\n",
            "Epoch 17/20\n",
            "1478/1478 [==============================] - 1714s 1s/step - loss: 0.6298 - masked_accuracy: 0.8249 - val_loss: 1.2405 - val_masked_accuracy: 0.7420\n",
            "Epoch 18/20\n",
            "1478/1478 [==============================] - 1716s 1s/step - loss: 0.6040 - masked_accuracy: 0.8310 - val_loss: 1.2126 - val_masked_accuracy: 0.7494\n",
            "Epoch 19/20\n",
            "1478/1478 [==============================] - 1727s 1s/step - loss: 0.5806 - masked_accuracy: 0.8365 - val_loss: 1.2740 - val_masked_accuracy: 0.7455\n",
            "Epoch 20/20\n",
            "1478/1478 [==============================] - 1830s 1s/step - loss: 0.5583 - masked_accuracy: 0.8415 - val_loss: 1.2414 - val_masked_accuracy: 0.7504\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "history = transformer.fit(MyBatchIterator_train,\n",
        "                epochs=20,\n",
        "                steps_per_epoch = BATCHES_PER_EPOCH,\n",
        "                validation_steps = BATCHES_PER_EPOCH_VAL,\n",
        "                validation_data=MyBatchIterator_val,\n",
        "                callbacks = [checkpoint_callback])\n",
        "\n",
        "display_history(history)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be easily noticed an overfit.  \n",
        "I tried to tackle this problem by trying several different hyperparameters set-ups and by augmenting datas without any improvements. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s now run the inference dynamic one token at a time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_TOKENS = 32\n",
        "\n",
        "class shuffle_inverter_class(tf.Module):\n",
        "  def __init__(self, transformer):\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, shuffled_sentence, max_length=MAX_TOKENS):\n",
        "\n",
        "    encoder_input = shuffled_sentence \n",
        "\n",
        "    output = np.ones(shape = (1,1))*2 # adding <start>\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      predictions = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "      # Select the last token from the `seq_len` dimension.\n",
        "      predicted = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "\n",
        "      predicted_id = np.argmax(predicted, axis=-1)\n",
        "\n",
        "      # Concatenate the `predicted_id` to the output which is given to the\n",
        "      # decoder as its input.\n",
        "      output = np.concatenate((output, predicted_id), axis=1)\n",
        "\n",
        "      if predicted_id == 3:\n",
        "        break\n",
        "\n",
        "    reordered_sentence = output\n",
        "\n",
        "    return reordered_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "shuffled_sen = np.expand_dims(x_test[0], axis=0)\n",
        "results = np.ones(shape=(1,32))*2\n",
        "\n",
        "shuffle_inverter = shuffle_inverter_class(transformer)\n",
        "shuffle_inverter(shuffled_sen)\n",
        "\n",
        "transformer.load_weights('transformer_weights_and_biases.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ulpTRdrF_huh"
      },
      "outputs": [],
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def score_fun(s,p):\n",
        "  match = SequenceMatcher(None, s, p).find_longest_match()\n",
        "  #print(match.size)\n",
        "  return (match.size/max(len(s),len(p)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing sentence n°0"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing sentence n°2999\n",
            "The average score computed over 3000 sentences is: 0.4857659387443567\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "avg_score = 0\n",
        "nn = 3000\n",
        "\n",
        "def score_generator():\n",
        "    while True:\n",
        "        sen_indices = [ii for ii in range(x_test.shape[0])]\n",
        "        random.shuffle(sen_indices) \n",
        "        sen_indices = sen_indices[:nn]\n",
        "\n",
        "        for index in sen_indices:\n",
        "            original_sentence = c_test[index]\n",
        "            shuffled_sentence = np.expand_dims(x_test[index], axis = 0)\n",
        "            ordered_sentence = shuffle_inverter(shuffled_sentence)[0]\n",
        "\n",
        "            original = tokenizer.sequences_to_texts([original_sentence[original_sentence > 3]])[0]\n",
        "            ordered = tokenizer.sequences_to_texts([ordered_sentence[ordered_sentence > 3]])[0]\n",
        "\n",
        "            score = score_fun(original, ordered)\n",
        "            yield score\n",
        "\n",
        "score_gen = iter(score_generator())\n",
        "for ii in range(nn):\n",
        "    sys.stdout.write(f\"\\rProcessing sentence n°{ii}\")\n",
        "    sys.stdout.flush()\n",
        "    avg_score += next(score_gen)/nn\n",
        "\n",
        "print(f\"\\nThe average score computed over 3000 sentences is: {avg_score}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4fwo7xj4GBW1"
      },
      "source": [
        "# What to deliver"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i6uITuxOGHfJ"
      },
      "source": [
        "You are supposed to deliver a single notebook, suitably commented. \n",
        "The notebook should describe a single model, although you may briefly discuss additional attempts you did.\n",
        "\n",
        "The notebook should contain a full trace of the training. \n",
        "Weights should be made available on request.\n",
        "\n",
        "You must also give a clear assesment of the performance of the model, computed with the metric that has been given to you.\n",
        "\n",
        "# Good work!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
